{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reviewer uihP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- W1: I am not quite sure about one of the key assumptions made in this paper: \"the non-causal substructure shares a similar pattern across graphs within a client but differs significantly among the clients\". Could you explain more why this assumption is valid? Or why it should be the case in real-world applications? To me, this is not clear.\n",
    "\n",
    "- R1: Thanks for bringing it up. The problem setting studied in this paper is derived from graph \n",
    "\n",
    "---\n",
    "\n",
    "- W2: Besides the above point, the definition of causal substructure is also not clear. If we have the above assumption, how we can decide the causal/non-causal parts given any multiple graphs? And motivated by this assumption, how do we design the component accordingly? In my view, the edge generation only ensures the similarity within a\n",
    "\n",
    "- R2: Thanks for pointing out this concern. Like introduced in many existing studies [16, 39, 63], the causal substructure is a subgraph which invariantly determines a graph's label. We also illustrate this in the toy example with Figure 1. We would like to clarify that **our method does NOT decide which part in a graph is causal or non-causal**. The goal of our method is to augment graphs by properly injecting VNs into graphs so that we can obtain identical embeddings of graphs with different non-causal substructure across clients.\n",
    "\n",
    "---\n",
    "\n",
    "- W3: For VNs collapsing, in my view, it prevents VNs from having similar features. But could you explain why they cannot have similar features? In my view, they can still reside in different parts of the graphs, thus have different neighboring structures. Then, they can still contribute differently.\n",
    "\n",
    "- R3: Thanks for pointing it out. We can understand the motivation of collapse prevention in both a) theoretical and b) intuitive ways. a) First, if we hope to obtain a theoretically identical embedding of two different graphs $\\mathcal{G}$ and $\\mathcal{G}'$ shown in Theorem 1, one condition is that $Q$ must have full row rank, i.e., $rank(Q)=M$. It means that multiple VNs cannot collapse to fewer nodes (line 277). b) Second, considering the toy example in Figure 1, we intuitively hope to learn three VNs for Circle, Pentagon, and Square, respectively. If the three VNs collapse to one single node, it will be much harder to obtain identical desirable graphs like shown in Figure 1. Therefore, learning multiple VNs is the most straightforward and reasonable way in our design."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reviewer R1ZR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- W1: The proposed method seems to be a simple extension of existing graph data argumentation methods. There are multiple studies apply similar method like synthetic nodes (GraphMixup; CATSMOTE, Mathematics'22) or edge generation (GraphMOTE, WSDM'21).\n",
    "\n",
    "- R1: We would like to argue that **our graph augmentation-based method is fundamentally different from the mentioned methods with synthetic nodes and edge generation.** To better understand their differences, we compare them as follows.\n",
    "\n",
    "|                |   The mentioned studies   |        Our method      |\n",
    "|-----------------------|-----------------------------|-----------------------------|\n",
    "| Downstream task level |      Node level        |        Graph level     |\n",
    "| Problem setting     |     Centralized        |         Federated     |\n",
    "|   Challenge       |  Imbalanced node labels    | Distribution shifts in FGL  |\n",
    "|   Intuition of generated nodes       |  Increase the number of labeled nodes    | Modify graphs to mitigate the impact of non-causal substructures  |\n",
    "|  Way to obtain generated node features      | Interpolation/mixup of existing nodes    |  Directly learn features of virtual nodes   |\n",
    "| Way to obtain generated edges       |  Learn an edge generator by real edge reconstruction    |  Directly learn score vectors by personalized edge generators          |\n",
    "| Optimization loss                  |  node classification loss + reconstruction loss     | graph prediction loss + decoupling loss + score-contrastive loss |\n",
    "\n",
    "From the above table, we conclude that our method is completely different from the mentioned methods regarding every aforementioned point. \n",
    "\n",
    "---\n",
    "\n",
    "- W2: Theorem 1 seems to be incorrect. Theorem 1 says that, given a GNN and two graph inputs, with the virtual nodes added and a delicate selection of their edges, two graphs can produce the same node embeddings. However, in the proof, the missing condition for eq. 18 -> eq. 17 is that $M \\geq d_x$, i.e., the number of virtual nodes should be at least equal to the node embedding dimension. This is a very strong assumption and is missing in the manuscript. Also, in the experiments, the number of virtual nodes are chosen to be $\\leq 100$ but the node embedding dimension is 100.\n",
    "\n",
    "- R2: We would like to clarify that the only condition about $Q \\in \\mathbb{R}^{M\\times d_x}$ in Theorem 1 is that there exists a right inverse matrix of $Q$. This condition requires that $Q$ must have full row rank, i.e., $rank(Q)=M$. **It implies that $M \\leq d_x$, instead of $M \\geq d_x$**. Therefore, we do not need to concern the number of virtual nodes $M$. \n",
    "\n",
    "---\n",
    "\n",
    "- Q1: The edge generator also has a large amount of parameters. Why not average it in global epoch but instead keep it local?\n",
    "\n",
    "- A1: Thanks for bringing this up. We would like to emphasize that our method aims to perform client-specific graph augmentation for each client. Our method achieves this by letting each client determine how virtual nodes connect its local graphs in a personalized manner. In other words, the edge generator in a client only needs to be trained over the local graphs in the client. Therefore, the edge generator is kept locally in our method. In addition, personalized edge generators can avoid additional communication cost, which is a major concern in federated learning.\n",
    "\n",
    "---\n",
    "\n",
    "- Q2: As the proposed method aims to reduce the distribution shift among different clients, is there any demonstration on the graph data distribution before/after using the proposed methods?\n",
    "\n",
    "- A2: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reviewer ppoH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- W1: The related work about federated graph learning lacks of important works about the graph learning challenges in federated learning.\n",
    "\n",
    "- R1: \n",
    "\n",
    "---\n",
    "\n",
    "- W2: The distribution shift over clients is not clear. No clear definitions are presented to formulate this new graph challenge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reviewer FZdY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- W1: The paper focuses on graph-level property prediction in FGL and assumes different clients share the same label space and common causal substructures. The assumption is strong and limits the application of the proposed method in real applications that contain diverse graph data in different clients.\n",
    "\n",
    "- R1: \n",
    "\n",
    "---\n",
    "- Q1: Can the proposed method be applied to the scenario where each client contains graphs from diverse domains?\n",
    "\n",
    "- A1: \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "- Q2: In the related work section, the authors should include more works on federated learning and contrastive learning in FL, to make this section more comprehensive.\n",
    "\n",
    "- A2: Thanks for your suggestion. We will add more related works in the revised version of our paper.\n",
    "\n",
    "---\n",
    "\n",
    "- Q3: In table 1, there are five settings. However, the caption says there are six settings.\n",
    "\n",
    "- A3: Thanks for pointing out this typo. We will correct this in our revised version once we can update our paper.\n",
    "\n",
    "---\n",
    "\n",
    "- Q4: For Eq (8), the definitions and calculations of $s_{local}$ and $s_{global}$ are not clear.\n",
    "\n",
    "- A4: \n",
    "\n",
    "---\n",
    "\n",
    "- Q5: The authors should further provide the time complexity introduced by the virtual nodes.\n",
    "\n",
    "- A5: \n",
    "\n",
    "---\n",
    "\n",
    "- Q6: The authors are encouraged to include discussions on graph explanation and graph out-of-distribution generalization as they share similar motivation with the paper i.e., focusing on the causal substructure of different graphs.\n",
    "\n",
    "- A6: Thanks for your suggestion. We would like to emphasize that we already included discussions on related studies about graph OOD generalization in Introduction (line 50-53). Although these studies attempt to grapple with the distribution shift of graph data from multiple sources for graph property prediction, these approaches are inapplicable to FGL since they require the multi-source graph data collected centrally in a mini-batch when training GNNs. This motivates us to propose our method in FGL. We will add related studies about  graph explanation in our revised version once we can update our paper.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reviewer CexK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
